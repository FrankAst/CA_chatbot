{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/gugui/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/gugui/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Descargar recursos necesarios\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función de preprocesamiento de texto\n",
    "\n",
    "def preprocesar_texto(texto):\n",
    "    # Convertir a minúsculas\n",
    "    texto = texto.lower()\n",
    "\n",
    "    # Eliminar las stopwords\n",
    "    stop_words = set(stopwords.words('spanish')) \n",
    "    palabras = texto.split()\n",
    "    palabras = [palabra for palabra in palabras if palabra not in stop_words]\n",
    "\n",
    "    # Lematización\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    palabras_lematizadas = [lemmatizer.lemmatize(palabra) for palabra in palabras]\n",
    "\n",
    "    # Volver a unir las palabras lematizadas en un solo texto\n",
    "    return ' '.join(palabras_lematizadas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importa JSON\n",
    "import json\n",
    "\n",
    "# Cargar el archivo JSON\n",
    "with open(\"grafo_hierarquia.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    grafo_json = json.load(file)\n",
    "\n",
    "# Crear una lista de nodos con categorías, subcategorías y productos\n",
    "nodos = []\n",
    "for cultivo, data in grafo_json.items():\n",
    "    nodos.append(cultivo)  # Agregar el nombre del cultivo\n",
    "    if 'Productos' in data:\n",
    "        for producto in data['Productos']:\n",
    "            nodos.append(producto)  # Agregar cada producto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Cargar el modelo de embeddings de frases\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar el preprocesamiento y convertir los nodos procesados a embeddings\n",
    "nodos_procesados = [preprocesar_texto(nodo) for nodo in nodos]\n",
    "nodos_embeddings = model.encode(nodos_procesados)\n",
    "nodos_embeddings = np.array(nodos_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "index = faiss.IndexFlatL2(nodos_embeddings.shape[1])\n",
    "index.add(nodos_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consulta_rag(consulta, index, nodos, model, top_k=5):\n",
    "    # Generar el embedding de la consulta\n",
    "    consulta_embedding = model.encode([preprocesar_texto(consulta)])\n",
    "    \n",
    "    # Buscar en el índice los nodos más cercanos\n",
    "    D, I = index.search(np.array(consulta_embedding), k=top_k)  \n",
    "    \n",
    "    # Mostrar los resultados\n",
    "    resultados = []\n",
    "    for i in range(top_k):\n",
    "        nodo = nodos[I[0][i]]\n",
    "        distancia = D[0][i]\n",
    "        resultados.append((nodo, distancia))\n",
    "        print(f\"Resultado {i + 1}: {nodo} (Distancia: {distancia:.4f})\")\n",
    "    \n",
    "    return resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola, ¿cómo estás?\n",
      "\n",
      "I'm sorry, I'm sorry.\n",
      "\n",
      "I'm sorry, I'm sorry.\n",
      "\n",
      "I'm sorry, I'm sorry.\n",
      "\n",
      "I'm sorry, I'm sorry\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Cargar el modelo y el tokenizador de GPT-2\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Ejemplo de uso\n",
    "input_text = \"Hola, ¿cómo estás?\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Generar una predicción\n",
    "output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# Decodificar y mostrar el resultado\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión después de reducción manual: (384,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_49944/3978295796.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  consulta_embedding = torch.tensor(consulta_embedding)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Supongamos que consulta_embedding tiene una dimensión de 768\n",
    "consulta_embedding = torch.tensor(consulta_embedding)\n",
    "\n",
    "# Reducir la dimensión a 384\n",
    "consulta_embedding_384 = consulta_embedding[:384].numpy()  # Tomar solo los primeros 384 elementos\n",
    "\n",
    "print(f\"Dimensión después de reducción manual: {consulta_embedding_384.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consulta: Spodoptera\n",
      "Resultado 1: Imunit® (Distancia: 2517.7275)\n",
      "Resultado 2: Vid (Distancia: 2521.0815)\n",
      "Resultado 3: Kit Sistiva®  (Distancia: 2521.4136)\n",
      "Resultado 4: Kit Sistiva®  (Distancia: 2521.4136)\n",
      "Resultado 5: Bellis® (Distancia: 2521.8257)\n",
      "Respuesta generada: Imunit® Vid Kit Sistiva®  Kit Sistiva®  Bellis® Viva™  Vivi® Sostiva  Sotiva ® Vivo®\n",
      "\n",
      "Vivis™ VIVi™ Sustiva Vividiva\n",
      ".\n",
      ",\n",
      ":\n",
      "-\n",
      "The Vixen® is a brand new product that is designed to be used with the VIXEN®. The VXVIVI® has been designed for use with VVIXE® and VXXV® products. VxVixE is an innovative and innovative product designed specifically for the use of VxxVX®, VxxxV, and XxxX. It is intended to provide a high quality, high performance, low cost, non-toxic, environmentally friendly, safe, portable, easy to use, affordable, convenient, reliable, durable, eco-friendly, ergonomic, light weight, lightweight, compact, versatile, efficient,\n"
     ]
    }
   ],
   "source": [
    "# Definir el número de nodos más cercanos que deseas obtener\n",
    "top_k = 5\n",
    "\n",
    "# Buscar en el índice FAISS los 5 nodos más cercanos\n",
    "D, I = index.search(np.array([consulta_embedding_384]), k=top_k)  # Buscar los 5 nodos más cercanos\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(f\"Consulta: {consulta}\")\n",
    "for i in range(top_k):\n",
    "    print(f\"Resultado {i + 1}: {nodos[I[0][i]]} (Distancia: {D[0][i]:.4f})\")\n",
    "\n",
    "# Concatenar los nodos más cercanos para formar un contexto\n",
    "contexto = \" \".join([nodos[I[0][i]] for i in range(top_k)])\n",
    "\n",
    "# Tokenizar el contexto\n",
    "input_ids = tokenizer.encode(contexto, return_tensors=\"pt\")\n",
    "\n",
    "# Generar una respuesta\n",
    "outputs = model.generate(input_ids, max_length=200, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "\n",
    "# Decodificar y mostrar la respuesta generada\n",
    "respuesta = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Respuesta generada: {respuesta}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desde aca para adelante revisar!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Cargar el modelo y el tokenizador\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Definir la consulta\n",
    "consulta = \"Rachiplusia Nu\"  # O cualquier otro término que desees\n",
    "\n",
    "# Tokenizar la consulta\n",
    "inputs = tokenizer(consulta, return_tensors=\"pt\")\n",
    "\n",
    "# Obtener el embedding (la salida del modelo)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomar la representación del [CLS] token (generalmente el primero)\n",
    "consulta_embedding_384 = outputs.last_hidden_state[:, 0, :].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Ahora puedes continuar con la búsqueda en FAISS\u001b[39;00m\n\u001b[1;32m      5\u001b[0m top_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m----> 6\u001b[0m D, I \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconsulta_embedding_384\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Buscar los 5 nodos más cercanos\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Mostrar los resultados\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsulta: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconsulta\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/faiss/class_wrappers.py:327\u001b[0m, in \u001b[0;36mhandle_Index.<locals>.replacement_search\u001b[0;34m(self, x, k, params, D, I)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreplacement_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, k, \u001b[38;5;241m*\u001b[39m, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, D\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, I\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    301\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the k nearest neighbors of the set of vectors x in the index.\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \n\u001b[1;32m    303\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;124;03m        When not enough results are found, the label is set to -1\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m     n, d \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    328\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m d \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Asegurarte de que consulta_embedding_384 tenga la forma correcta (1, 384)\n",
    "consulta_embedding_384 = np.expand_dims(consulta_embedding_384, axis=0)\n",
    "\n",
    "# Ahora puedes continuar con la búsqueda en FAISS\n",
    "top_k = 5\n",
    "D, I = index.search(consulta_embedding_384, k=top_k)  # Buscar los 5 nodos más cercanos\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(f\"Consulta: {consulta}\")\n",
    "for i in range(top_k):\n",
    "    print(f\"Resultado {i + 1}: {nodos[I[0][i]]} (Distancia: {D[0][i]:.4f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar los nodos más cercanos para formar un contexto\n",
    "contexto = \" \".join([nodos[I[0][i]] for i in range(top_k)])\n",
    "\n",
    "# Tokenizar el contexto\n",
    "input_ids = tokenizer.encode(contexto, return_tensors=\"pt\")\n",
    "\n",
    "# Generar una respuesta\n",
    "outputs = model.generate(input_ids, max_length=200, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "\n",
    "# Decodificar y mostrar la respuesta generada\n",
    "respuesta = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Respuesta generada: {respuesta}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
